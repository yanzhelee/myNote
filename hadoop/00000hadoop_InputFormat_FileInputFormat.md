# Hadoop InputFormat之FileInputFormat

## FileInputFormat介绍

FileInputFormat是专门针对文件类型的数据源而设计的，也是一个抽象类，它提供两方面的作用：
1. 定义Job输入文件的静态方法
2. 为输入文件形成切片的通用实现

至于如何将切片中的数据转换为一条条的记录，则根据文件类型的不同交由具体的子类负责实现。下图是FileInputFormat类图：

![FileInputFormat](https://raw.githubusercontent.com/yanzhelee/myNote/master/images/hadoop/hadoop_InputFormat_FileInputFormat_1.png)

**FileInputFormat提供了四个静态方法用于定义Job的输入文件路径**
```java
// 用于添加一个输入路径
public static void addInputPath(Job job, Path path)
// 用于添加一批输入路径，路径之间用逗号分隔
public static void addInputPaths(Job job, String commaSeparatedPaths)
// 设置输入路径
public static void setInputPaths(Job job, Path... inputPaths)
// 设置输入路径，路径之间用逗号分隔
public static void setInputPaths(Job job, String commaSeparatedPaths)
```

*注意*
> addInputPath()的每一次调用都会将输入路径与原有值以逗号分隔进行拼接(不会覆盖原有的值)并保存至Job Configuration的属性`INPUT_DIR(mapreduce.input.fileinputformat.inputdir)`中。
>
> addInputPaths()方法是对addInputPath()方法的循环调用
>
> setInputPaths()方法有两个重载，用于设置一批输入路径，该方法用于一次性调用，每次调用都会覆盖之前的结果。该方法会替换Job Configuration的属性`INPUT_DIR(mapreduce.input.fileinputformat.inputdir)`中的值。
>
> 这个输入路径可以是文件也可以是目录。
>
> 目录中的内容不会被递归处理。实际上目录中应仅包含文件，如果目录中包含子目录，这些子目录就会当作文件进行处理，从而引发异常。如果我们不需要递归目录，我们可以通过File Pattern或者Filter告知FileInputFormat仅仅选取制定 目录中的文件；如果我们确实需要递归处理目录，则可以通过设置`mapreduce.input.fileinputformat.input.dir.recursive`为true。

**路径的过滤**
```java
/**
 * Set a PathFilter to be applied to the input paths for the map-reduce job.
 * @param job the job to modify
 * @param filter the PathFilter class use for filtering the input paths.
 */
public static void setInputPathFilter(Job job,
                                      Class<? extends PathFilter> filter) {
  job.getConfiguration().setClass(PATHFILTER_CLASS, filter,
                                  PathFilter.class);
}
```
指定一个PathFilter的具体实现类名称，用于保存Job Configuration属性`PATHFILTER_CLASS(maprecude.input.pathfilter.class)`中。

如果没有显示设置PathFilter，FileInputFormat会有一个默认的过滤器，用于过滤目录中的隐藏文件；如果我们显示设置PathFilter，则FileInputFormat的过滤器实则是一个过滤器链，而默认的过滤器回居于过滤器链的首部，优先被执行。

**切片的计算**
FileInputFormat生成切片的过程是由getSplits()方法实现的，源码如下：
```java
/**
 * 生成切片列表
 * @param job the job context
 * @throws IOException
 */
public List<InputSplit> getSplits(JobContext job) throws IOException {
  StopWatch sw = new StopWatch().start();
  long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
  long maxSize = getMaxSplitSize(job);

  List<InputSplit> splits = new ArrayList<InputSplit>();
  // 获取输入路径中的所有文件信息
  List<FileStatus> files = listStatus(job);
  // 迭代处理输入路径中的每一个文件，为每个文件生成切片
  for (FileStatus file: files) {
    Path path = file.getPath();
    long length = file.getLen();
    if (length != 0) {
      BlockLocation[] blkLocations;
      if (file instanceof LocatedFileStatus) {
        blkLocations = ((LocatedFileStatus) file).getBlockLocations();
      } else {
        FileSystem fs = path.getFileSystem(job.getConfiguration());
        blkLocations = fs.getFileBlockLocations(file, 0, length);
      }
      if (isSplitable(job, path)) {
        long blockSize = file.getBlockSize();
        // 得到切片大小
        long splitSize = computeSplitSize(blockSize, minSize, maxSize);

        long bytesRemaining = length;
        while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {
          int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
          splits.add(makeSplit(path, length-bytesRemaining, splitSize,
                      blkLocations[blkIndex].getHosts(),
                      blkLocations[blkIndex].getCachedHosts()));
          bytesRemaining -= splitSize;
        }

        if (bytesRemaining != 0) {
          int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
          splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,
                     blkLocations[blkIndex].getHosts(),
                     blkLocations[blkIndex].getCachedHosts()));
        }
      } else { // 不可切割
        splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(),
                    blkLocations[0].getCachedHosts()));
      }
    } else {
      // 如果文件的长度为0，则声称一个空的切片
      splits.add(makeSplit(path, 0, length, new String[0]));
    }
  }
  // Save the number of input files for metrics/loadgen
  job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());
  sw.stop();
  if (LOG.isDebugEnabled()) {
    LOG.debug("Total # of splits generated by getSplits: " + splits.size()
        + ", TimeTaken: " + sw.now(TimeUnit.MILLISECONDS));
  }
  return splits;
}
```
*下面介绍一下文件切片的形成过程*

*Step1* 获取文件块大小(blockSize),然后根据切片最小值，切片最大值计算文件对应的切片大小
> 计算公式：`splitSize = Math.max(minSize, Math.min(maxSize, blockSize));`

*Step2* 判断文件剩余大小(未切片的大小)是否满足继续进行切片的条件
> 判断条件：`((double) bytesRemaining)/splitSize > SPLIT_SLOP`是否为true
> 其中bytesRemaining初始值为文件长度，SPLIT_SLOP值为1.1且不可修改，即文件剩余大小为splitSize的1.1倍才会继续切片。

Step3 获取切片对应的数据块。
> 一个切片根据切片大小的不同，可能会包含若干个数据块，这里将第一个数据块的副本位置作为切片的存储位置。
>
> 切片在文件中的起始偏移量的计算公式是`offset = (n-1)*splitSize` n表示第几个切片
>
> 对于给定的切片的offset，getBlockIndex()方法就是计算文件的那个数据块的起止范围恰好包含offset，返回这个数据块在 数据块列表中的下标，计算流程如下 ：
```java
protected int getBlockIndex(BlockLocation[] blkLocations, long offset) {
  for (int i = 0 ; i < blkLocations.length; i++) {
    // is the offset inside this block?
    if ((blkLocations[i].getOffset() <= offset) &&
        (offset < blkLocations[i].getOffset() + blkLocations[i].getLength())){
      return i;
    }
  }
  BlockLocation last = blkLocations[blkLocations.length -1];
  long fileLength = last.getOffset() + last.getLength() -1;
  throw new IllegalArgumentException("Offset " + offset +
                                     " is outside of file (0.." +
                                     fileLength + ")");
}
```

Step4 根据下表对应的数据块信息构建一个FileSplit
> 根据FileSplit的信息，可以看出FileSplit并不实现保存数据，仅仅通过文件名称、起始偏移量、大小关联数据，并将对应数据的副本位置作为切片的存储位置进行MapTask的调度。
>
> 循环执行Step2、Step3、Step4直到文件生育大小无法满足切片条件。

Step5 将文件的剩余部分构建一个FileSplit

## FileInputFormat子类之TextInputFormat

TextInputFormat格式是最常见的文件输入格式，默认情况下就是采用改格式，源码如下：
```java
public class TextInputFormat extends FileInputFormat<LongWritable, Text> {

  /**
   * 返回的就是LineRecordReader实例
   */
  public RecordReader<LongWritable, Text>
    createRecordReader(InputSplit split,
                       TaskAttemptContext context) {
    String delimiter = context.getConfiguration().get(
        "textinputformat.record.delimiter");
    byte[] recordDelimiterBytes = null;
    if (null != delimiter)
      recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8);
    return new LineRecordReader(recordDelimiterBytes);
  }

  /**
   * 如果文件不是压缩格式，那么文件一定可切割
   * 如果是压缩格式，那么文件是否可切割取决于压缩格式
   */
  protected boolean isSplitable(JobContext context, Path file) {
    final CompressionCodec codec =
      new CompressionCodecFactory(context.getConfiguration()).getCodec(file);
    if (null == codec) {
      return true;
    }
    return codec instanceof SplittableCompressionCodec;
  }

}
```

在split的读取方面，它是将给定的切片按行读取 ，以行的首字节在文件中的偏移量作为key，以行的内容作为value传给map函数进行处理，这部分的逻辑是由它所创建并使用的LineRecordReader封装和实现的。

**疑问 ?**
> 如果一个行被切分到两个split里(这几乎是一定会发生的情况),TextInputFormat是如何处理的?

LineRecordReader会创建一个org.apache.hadoop.util.LineReader实例,并依赖这个LineReader的readLine方法来读取一行记录,下面简单介绍一下readLine()方法的实现。
> 是从buffer里读取数据,如果buffer里的数据读完了,先加载下一批数据到buffer。
>
> 在buffer中查找"行尾",将开始位置至行尾处的数据拷贝给str(也就是最后的Value).如果为遇到"行尾",继续加载新的数据到buffer进行查找.
>
> 关键点在于:给到buffer的数据是直接从文件中读取的,完全不会考虑是否超过了split的界限,而是一直读取到当前行结束为止。
>
> 按照readLine的上述行为,在遇到跨split的行时,会到下一个split继续读取数据直至行尾,那么下一个split怎么判定开头的一行有没有被上一个split的LineRecordReader读取过从而避免漏读或重复读取开头一行呢?
>
> 这方面LineRecordReader使用了一个简单而巧妙的方法:既然无法断定每一个split开始的一行是独立的一行还是被切断的一行的一部分,那就跳过每个split的开始一行(当然要除第一个split之外),从第二行开始读取,然后在到达split的结尾端时总是再多读一行,这样数据既能接续起来又避开了断行带来的麻烦.
>
> 至此,跨split的行读取的逻辑就完备了.如果引申地来看,这是map-reduce前期数据切分的一个普遍性问题,即不管我们用什么方式切分和读取一份大数据中的小部分,包括我们在实现自己的InputFormat时,都会面临在切分处数据时的连续性解析问题. 对此我们应该深刻地认识到:split最直接的现实作用是取出大数据中的一小部分给mapper处理,但这只是一种"逻辑"上的,"宏观"上的切分,在"微观"上,在split的首尾切分处,为了确保数据连续性,跨越split接续并拼接数据也是完全正当和合理的。

## FileInputFormat子类之KeyValueTextInputFormat

针对每一行只有key-Value的特殊文件，Hadoop提供了输入格式是KeyValueTextInputFormat。它的原理与TextInputFormat相似，只不过TextInputFormat是按行进行处理以每一行的首字节的偏移量作为key，以一整行内容作为value；而KeyValueTextInputFormat是以每行中的第一列为key，第二列为value。

下面是KeyValueTextInputFormat类的源码：
```java
public class KeyValueTextInputFormat extends FileInputFormat<Text, Text> {

  // 判断是否可切割
  protected boolean isSplitable(JobContext context, Path file) {
    final CompressionCodec codec =
      new CompressionCodecFactory(context.getConfiguration()).getCodec(file);
    if (null == codec) {
      return true;
    }
    return codec instanceof SplittableCompressionCodec;
  }
  // 使用的是KeyValueLineRecordReader阅读器
  public RecordReader<Text, Text> createRecordReader(InputSplit genericSplit,
      TaskAttemptContext context) throws IOException {

    context.setStatus(genericSplit.toString());
    return new KeyValueLineRecordReader(context.getConfiguration());
  }

}
```

### KeyValueTextInputFormat的使用

第一步：设置文件输入格式
  > `job.setInputFormatClass(KeyValueTextInputFormat.class); `

第二步：设置KeyValue之间分隔符
  > 默认情况下是按照制表符进行分割的自定义设置参考如下代码
  >
  > `conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, " "); `


## FileInputFormat子类之NLineInputFormat

默认情况下在对输入文件进行过拆分时，会按block的大小分成多个InputSplit,InputSplit的数量取决于block的大小和设置的切片最大值和最小值。每个MapTask处理一个InputSplit，InputSplit中有多少行记录就会调用多少次map函数。

如果使用NLineInputFormat，代表每个MapTask处理的InputSplit不再按block进行划分，而是按照NLineInputFormat指定的N来划分。即每个InputSplit中中只有N行记录数。同样InputSplit中有多少行记录就会调用多少次map函数。

## 参考博文

[http://www.cnblogs.com/yurunmiao/p/4514017.html](http://www.cnblogs.com/yurunmiao/p/4514017.html)

[http://blog.csdn.net/bluishglc/article/details/9380087](http://blog.csdn.net/bluishglc/article/details/9380087)
